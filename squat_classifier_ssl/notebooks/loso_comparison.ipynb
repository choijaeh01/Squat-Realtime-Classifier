{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# LOSO 실험 비교 분석\n\n여러 체크포인트(`loso_*`)에서 생성된 LOSO 결과를 한 번에 불러와 성능을 비교합니다. 필요 시 `RUNS` 설정을 수정해 분석 대상을 조정하세요.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from pathlib import Path\nimport json\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\n\nsns.set_theme(style=\"whitegrid\", context=\"notebook\")\n\nNOTEBOOK_DIR = Path.cwd()\nif (NOTEBOOK_DIR / \"checkpoints\").exists():\n    PROJECT_ROOT = NOTEBOOK_DIR\nelif (NOTEBOOK_DIR.parent / \"checkpoints\").exists():\n    PROJECT_ROOT = NOTEBOOK_DIR.parent\nelse:\n    raise FileNotFoundError(\"checkpoints 디렉터리를 찾을 수 없습니다. 노트북 위치를 확인해 주세요.\")\n\nCHECKPOINT_BASE = PROJECT_ROOT / \"checkpoints\"\nCLASS_LABELS = {\n    0: \"Correct\",\n    1: \"Knee Valgus\",\n    2: \"Butt Wink\",\n    3: \"Excessive Lean\",\n    4: \"Partial Squat\",\n}\n\nRUNS = [\n    {\"id\": \"loso_nossl\", \"name\": \"No-SSL\", \"path\": CHECKPOINT_BASE / \"loso_nossl\"},\n    {\"id\": \"loso_sc\", \"name\": \"SimCLR\", \"path\": CHECKPOINT_BASE / \"loso_sc\"},\n    {\"id\": \"loso_ss\", \"name\": \"SimSiam\", \"path\": CHECKPOINT_BASE / \"loso_ss\"},\n    {\"id\": \"loso_ss_focalyes\", \"name\": \"SimSiam+Focal\", \"path\": CHECKPOINT_BASE / \"loso_ss_focalyes\"},\n]\n\nprint(f\"프로젝트 루트: {PROJECT_ROOT}\")\nprint(\"분석 대상:\")\nfor run in RUNS:\n    print(f\"  - {run['name']}: {run['path']}\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def find_report_file(subject_dir: Path):\n    candidates = sorted(subject_dir.glob(\"classification_report*.json\"))\n    return candidates[0] if candidates else None\n\nfold_rows = []\nclass_rows = []\nprediction_frames = []\nmissing_reports = []\n\nfor order, run in enumerate(RUNS):\n    run_dir = run[\"path\"]\n    if not run_dir.exists():\n        print(f\"[경고] {run['name']} 경로가 없어 건너뜁니다: {run_dir}\")\n        continue\n\n    subject_dirs = sorted([\n        p for p in run_dir.iterdir() if p.is_dir() and p.name.startswith(\"subject\")\n    ], key=lambda p: int(''.join(filter(str.isdigit, p.name)) or 0))\n\n    for subject_dir in subject_dirs:\n        subject_id = subject_dir.name\n        report_path = find_report_file(subject_dir)\n        if report_path is None:\n            missing_reports.append((run[\"name\"], subject_id))\n            continue\n\n        with report_path.open() as f:\n            report = json.load(f)\n\n        macro_avg = report.get(\"macro avg\", {})\n        accuracy = report.get(\"accuracy\", np.nan)\n        macro_precision = macro_avg.get(\"precision\", np.nan)\n        macro_recall = macro_avg.get(\"recall\", np.nan)\n        macro_f1 = macro_avg.get(\"f1-score\", np.nan)\n\n        fold_rows.append(\n            {\n                \"run_id\": run[\"id\"],\n                \"run_name\": run[\"name\"],\n                \"run_order\": order,\n                \"subject\": subject_id,\n                \"Accuracy\": accuracy,\n                \"Macro Precision\": macro_precision,\n                \"Macro Recall\": macro_recall,\n                \"Macro F1\": macro_f1,\n                \"Balanced Accuracy\": macro_recall,\n            }\n        )\n\n        for cls_key, metrics in report.items():\n            if not cls_key.isdigit():\n                continue\n            cls_id = int(cls_key)\n            class_rows.append(\n                {\n                    \"run_id\": run[\"id\"],\n                    \"run_name\": run[\"name\"],\n                    \"run_order\": order,\n                    \"subject\": subject_id,\n                    \"class_id\": cls_id,\n                    \"class_name\": CLASS_LABELS.get(cls_id, f\"Class {cls_id}\"),\n                    \"precision\": metrics.get(\"precision\", np.nan),\n                    \"recall\": metrics.get(\"recall\", np.nan),\n                    \"f1\": metrics.get(\"f1-score\", np.nan),\n                    \"support\": metrics.get(\"support\", np.nan),\n                }\n            )\n\n        prediction_path = subject_dir / \"sample_predictions.csv\"\n        if prediction_path.exists():\n            df = pd.read_csv(prediction_path)\n            df[\"run_name\"] = run[\"name\"]\n            df[\"run_id\"] = run[\"id\"]\n            df[\"run_order\"] = order\n            df[\"subject\"] = subject_id\n            prediction_frames.append(df)\n\nfold_df = pd.DataFrame(fold_rows)\nclass_df = pd.DataFrame(class_rows)\npred_df = pd.concat(prediction_frames, ignore_index=True) if prediction_frames else pd.DataFrame()\n\nprint(f\"총 fold 레코드 수: {len(fold_df)}\")\nprint(f\"총 클래스 레코드 수: {len(class_df)}\")\nprint(f\"총 예측 샘플 수: {len(pred_df)}\")\nif missing_reports:\n    print(\"[경고] 누락된 리포트:\")\n    for run_name, subject in missing_reports:\n        print(f\"  - {run_name}: {subject}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. 실험별 성능 요약\n각 지표별로 실험 평균/표준편차/최고/최저를 비교합니다.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "if fold_df.empty:\n    raise RuntimeError(\"fold_df가 비어 있습니다. 상단 설정을 확인하세요.\")\n\nsummary_rows = []\nmetrics = [\"Accuracy\", \"Macro F1\", \"Balanced Accuracy\"]\n\nfor metric in metrics:\n    grouped = fold_df.groupby([\"run_order\", \"run_name\"])[metric]\n    for (order, run_name), values in grouped:\n        summary_rows.append(\n            {\n                \"Run\": run_name,\n                \"Metric\": metric,\n                \"Mean\": values.mean(),\n                \"Std\": values.std(ddof=1),\n                \"Min\": values.min(),\n                \"Max\": values.max(),\n                \"Mean ± Std\": f\"{values.mean():.4f} ± {values.std(ddof=1):.4f}\",\n            }\n        )\n\nrun_summary = pd.DataFrame(summary_rows).sort_values([\"Metric\", \"Run\"])\ndisplay(run_summary.style.format({\"Mean\": \"{:.3f}\", \"Std\": \"{:.3f}\", \"Min\": \"{:.3f}\", \"Max\": \"{:.3f}\"}))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. 피험자별 Macro F1 비교\n각 실험의 피험자별 Macro F1을 테이블과 히트맵으로 확인합니다.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "macro_f1_pivot = fold_df.pivot(index=\"subject\", columns=\"run_name\", values=\"Macro F1\")\ndisplay(macro_f1_pivot.sort_index().style.format(\"{:.3f}\"))\n\nplt.figure(figsize=(10, 4 + 0.5 * len(macro_f1_pivot)))\nsns.heatmap(\n    macro_f1_pivot.sort_index(),\n    annot=True,\n    fmt=\".2f\",\n    cmap=\"YlGnBu\",\n    vmin=0,\n    vmax=1,\n    cbar_kws={\"label\": \"Macro F1\"},\n)\nplt.title(\"Subject vs. Run Macro F1\")\nplt.ylabel(\"Subject\")\nplt.xlabel(\"Run\")\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. 실험별 Macro F1 분포\n피험자 분포를 박스플롯으로 비교합니다.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "plt.figure(figsize=(8, 4))\nsns.boxplot(data=fold_df, x=\"run_name\", y=\"Macro F1\", palette=\"Set2\")\nsns.swarmplot(data=fold_df, x=\"run_name\", y=\"Macro F1\", color=\"black\", alpha=0.6)\nplt.ylim(0, 1)\nplt.xlabel(\"Run\")\nplt.ylabel(\"Macro F1\")\nplt.title(\"Macro F1 distribution per Run\")\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. 클래스별 Precision / Recall / F1 비교\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "class_avg = (\n    class_df.groupby([\"run_order\", \"run_name\", \"class_name\"])[[\"precision\", \"recall\", \"f1\"]]\n    .mean()\n    .reset_index()\n    .sort_values([\"class_name\", \"run_order\"])\n)\ndisplay(class_avg.style.format({\"precision\": \"{:.3f}\", \"recall\": \"{:.3f}\", \"f1\": \"{:.3f}\"}))\n\nplt.figure(figsize=(10, 5))\nsns.barplot(data=class_avg, x=\"class_name\", y=\"f1\", hue=\"run_name\", palette=\"viridis\")\nplt.ylim(0, 1)\nplt.ylabel(\"F1-Score\")\nplt.xlabel(\"Class\")\nplt.title(\"Class-wise F1 across Runs\")\nplt.legend(title=\"Run\")\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Confusion Matrix 비교 (모든 fold 예측 포함)\n각 실험의 `sample_predictions.csv`를 합쳐 혼동행렬을 생성합니다.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "if pred_df.empty:\n    print(\"sample_predictions.csv가 없어 혼동행렬을 생성할 수 없습니다.\")\nelse:\n    pred_df = pred_df.copy()\n    pred_df[\"true_idx\"] = pred_df[\"true_idx\"].astype(int)\n    pred_df[\"pred_idx\"] = pred_df[\"pred_idx\"].astype(int)\n    class_ids = sorted(CLASS_LABELS.keys())\n    n_runs = pred_df[\"run_name\"].nunique()\n    fig_cols = min(2, n_runs)\n    fig_rows = int(np.ceil(n_runs / fig_cols))\n    fig, axes = plt.subplots(fig_rows, fig_cols, figsize=(6 * fig_cols, 5 * fig_rows))\n    if not isinstance(axes, np.ndarray):\n        axes = np.array([axes])\n    axes = axes.flatten()\n\n    for ax in axes[n_runs:]:\n        ax.axis(\"off\")\n\n    for ax, (run_name, group) in zip(axes, pred_df.groupby(\"run_name\")):\n        cm = (\n            pd.crosstab(group[\"true_idx\"], group[\"pred_idx\"])\n            .reindex(index=class_ids, columns=class_ids, fill_value=0)\n        )\n        cm.index = [CLASS_LABELS[i] for i in class_ids]\n        cm.columns = cm.index\n        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"rocket_r\", cbar=False, ax=ax)\n        ax.set_title(run_name)\n        ax.set_ylabel(\"True\")\n        ax.set_xlabel(\"Pred\")\n\n    fig.suptitle(\"Confusion Matrices by Run\", y=1.02, fontsize=14)\n    plt.tight_layout()\n    plt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. 추가 아이디어\n- 상단 `RUNS` 리스트에 새로운 체크포인트 경로를 추가하면 자동으로 분석됩니다.\n- 필요한 경우 `metrics` 리스트를 수정해 다른 지표(예: Weighted F1)를 비교할 수 있습니다.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}