{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# LOSO 검증 결과 분석\n\n`checkpoints/loso_nossl`에 저장된 LOSO fold별 결과를 불러와서 성능 지표, 요약 통계, 클래스별 지표, 시각화를 한 번에 확인할 수 있는 노트북입니다.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from pathlib import Path\nimport json\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\n\nsns.set_theme(style=\"whitegrid\", context=\"notebook\")\n\nNOTEBOOK_DIR = Path.cwd()\nif (NOTEBOOK_DIR / \"checkpoints\").exists():\n    PROJECT_ROOT = NOTEBOOK_DIR\nelif (NOTEBOOK_DIR.parent / \"checkpoints\").exists():\n    PROJECT_ROOT = NOTEBOOK_DIR.parent\nelse:\n    raise FileNotFoundError(\"checkpoints 디렉터리를 찾을 수 없습니다. 노트북 위치를 확인하세요.\")\n\nCHECKPOINT_DIR = PROJECT_ROOT / \"checkpoints\" / \"loso_nossl\"\nCLASS_LABELS = {\n    0: \"Correct\",\n    1: \"Knee Valgus\",\n    2: \"Butt Wink\",\n    3: \"Excessive Lean\",\n    4: \"Partial Squat\",\n}\n\nprint(f\"분석 대상 디렉터리: {CHECKPOINT_DIR}\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "subject_dirs = sorted(\n    [p for p in CHECKPOINT_DIR.iterdir() if p.is_dir() and p.name.startswith(\"subject\")],\n    key=lambda p: int(p.name.replace(\"subject\", \"\")),\n)\n\nfold_records = []\nclass_records = []\nmissing_reports = []\n\nfor subject_dir in subject_dirs:\n    subject_id = subject_dir.name\n    report_files = sorted(subject_dir.glob(\"classification_report*.json\"))\n    if not report_files:\n        missing_reports.append(subject_id)\n        continue\n\n    with open(report_files[0], \"r\") as f:\n        report = json.load(f)\n\n    macro_avg = report.get(\"macro avg\", {})\n    accuracy = report.get(\"accuracy\", np.nan)\n    macro_precision = macro_avg.get(\"precision\", np.nan)\n    macro_recall = macro_avg.get(\"recall\", np.nan)\n    macro_f1 = macro_avg.get(\"f1-score\", np.nan)\n\n    fold_records.append(\n        {\n            \"subject\": subject_id,\n            \"Accuracy\": accuracy,\n            \"Macro Precision\": macro_precision,\n            \"Macro Recall\": macro_recall,\n            \"Macro F1\": macro_f1,\n            \"Balanced Accuracy\": macro_recall,\n        }\n    )\n\n    for cls_key, metrics in report.items():\n        if not cls_key.isdigit():\n            continue\n        cls_id = int(cls_key)\n        class_records.append(\n            {\n                \"subject\": subject_id,\n                \"class_id\": cls_id,\n                \"class_name\": CLASS_LABELS.get(cls_id, f\"Class {cls_id}\"),\n                \"precision\": metrics.get(\"precision\", np.nan),\n                \"recall\": metrics.get(\"recall\", np.nan),\n                \"f1\": metrics.get(\"f1-score\", np.nan),\n                \"support\": metrics.get(\"support\", np.nan),\n            }\n        )\n\nfold_df = pd.DataFrame(fold_records).set_index(\"subject\").sort_index()\nclass_df = pd.DataFrame(class_records)\n\nprint(f\"총 {len(fold_df)}개 subject 로드 완료\")\nif missing_reports:\n    print(\"Missing reports:\", \", \".join(missing_reports))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. LOSO Fold별 성능 표\nAccuracy, Macro F1, Balanced Accuracy(=클래스별 recall 평균)을 정리했습니다.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "fold_metrics = fold_df[[\"Accuracy\", \"Macro F1\", \"Balanced Accuracy\"]].copy()\ndisplay(fold_metrics.style.format(\"{:.3f}\"))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. 전체 통계 요약\n각 지표에 대해 mean ± std, 최악/최고 subject 정보를 제공합니다.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def summarize_metric(df: pd.DataFrame, metric: str) -> dict:\n    values = df[metric]\n    return {\n        \"Metric\": metric,\n        \"Mean\": values.mean(),\n        \"Std\": values.std(ddof=1),\n        \"Mean ± Std\": f\"{values.mean():.4f} ± {values.std(ddof=1):.4f}\",\n        \"Min (worst subject)\": f\"{values.min():.4f} ({values.idxmin()})\",\n        \"Max (best subject)\": f\"{values.max():.4f} ({values.idxmax()})\",\n    }\n\nsummary_table = pd.DataFrame(\n    [summarize_metric(fold_metrics, metric) for metric in fold_metrics.columns]\n)\ndisplay(summary_table)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. 클래스별 Precision / Recall / F1 (전체 피험자 평균)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "class_avg = (\n    class_df.groupby([\"class_id\", \"class_name\"])[[\"precision\", \"recall\", \"f1\"]]\n    .mean()\n    .reset_index()\n    .sort_values(\"class_id\")\n)\ndisplay(class_avg.style.format({\"precision\": \"{:.3f}\", \"recall\": \"{:.3f}\", \"f1\": \"{:.3f}\"}))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. 피험자별 클래스 F1 표 & 히트맵\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "class_order = [CLASS_LABELS[k] for k in sorted(CLASS_LABELS.keys())]\nsubject_class_f1 = (\n    class_df.pivot(index=\"subject\", columns=\"class_name\", values=\"f1\")\n    .reindex(fold_metrics.index)\n    [class_order]\n)\n\ndisplay(subject_class_f1.style.format(\"{:.3f}\"))\n\nplt.figure(figsize=(10, 4 + len(subject_class_f1) * 0.5))\nsns.heatmap(\n    subject_class_f1,\n    annot=True,\n    fmt=\".2f\",\n    cmap=\"YlGnBu\",\n    vmin=0,\n    vmax=1,\n    cbar_kws={\"label\": \"F1-Score\"},\n)\nplt.title(\"Subject vs. Class F1 Heatmap\")\nplt.ylabel(\"Subject\")\nplt.xlabel(\"Class\")\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. 피험자별 전반 성능 (Macro F1) Barplot\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "macro_f1_mean = fold_metrics[\"Macro F1\"].mean()\nplot_df = fold_metrics.reset_index()\n\nplt.figure(figsize=(8, 4))\nsns.barplot(data=plot_df, x=\"subject\", y=\"Macro F1\", palette=\"Blues_d\")\nplt.axhline(macro_f1_mean, color=\"red\", linestyle=\"--\", label=f\"Mean = {macro_f1_mean:.3f}\")\nplt.ylim(0, 1)\nplt.ylabel(\"Macro F1-Score\")\nplt.xlabel(\"Subject\")\nplt.title(\"Subject-wise Macro F1\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. LOSO 전체 Confusion Matrix\n모든 fold의 `sample_predictions.csv`를 합쳐 구성했습니다.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "prediction_frames = []\nfor subject_dir in subject_dirs:\n    csv_path = subject_dir / \"sample_predictions.csv\"\n    if csv_path.exists():\n        df = pd.read_csv(csv_path)\n        df[\"subject\"] = subject_dir.name\n        prediction_frames.append(df)\n\nif not prediction_frames:\n    raise FileNotFoundError(\"sample_predictions.csv 파일을 찾을 수 없습니다.\")\n\npredictions = pd.concat(prediction_frames, ignore_index=True)\npredictions[\"true_idx\"] = predictions[\"true_idx\"].astype(int)\npredictions[\"pred_idx\"] = predictions[\"pred_idx\"].astype(int)\n\nclass_ids = sorted(CLASS_LABELS.keys())\nconf_matrix = (\n    pd.crosstab(predictions[\"true_idx\"], predictions[\"pred_idx\"])\n    .reindex(index=class_ids, columns=class_ids, fill_value=0)\n)\nconf_matrix.index = [CLASS_LABELS[i] for i in class_ids]\nconf_matrix.columns = conf_matrix.index\n\ndisplay(conf_matrix)\n\nplt.figure(figsize=(7, 6))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"rocket_r\", cbar=False)\nplt.title(\"Confusion Matrix (All LOSO Folds)\")\nplt.ylabel(\"True label\")\nplt.xlabel(\"Predicted label\")\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. 클래스별 F1 Barplot\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "plt.figure(figsize=(8, 4))\nsns.barplot(data=class_avg, x=\"class_name\", y=\"f1\", palette=\"viridis\")\nplt.ylim(0, 1)\nplt.ylabel(\"F1-Score\")\nplt.xlabel(\"Class\")\nplt.title(\"Class-wise F1 (Averaged over subjects)\")\nfor idx, value in enumerate(class_avg[\"f1\"]):\n    plt.text(idx, value + 0.01, f\"{value:.2f}\", ha=\"center\", va=\"bottom\")\nplt.tight_layout()\nplt.show()\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}